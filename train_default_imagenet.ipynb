{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Device: cpu\n"
     ]
    }
   ],
   "source": [
    "SEED = 1\n",
    "\n",
    "# Use GPU if available, otherwise CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print('Current Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageNet mean and std\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "IMAGENET_TRANSFORMS = transforms.Compose(\n",
    "    [\n",
    "        # ResNet18 will resize images to 256x256, then takes a central crop of 224x224\n",
    "        transforms.Resize((256,256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = torchvision.datasets.Imagenette(root='./datasets/imagenette', split='train', download=False, transform=IMAGENET_TRANSFORMS)\n",
    "data_val = torchvision.datasets.Imagenette(root='./datasets/imagenette', split='val', download=False, transform=IMAGENET_TRANSFORMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagenetteDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, dataset, indices, transform=None):\n",
    "        self.transforms = transform\n",
    "        self.data = torch.utils.data.Subset(dataset, indices)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.data[index]\n",
    "        if self.transforms != None:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        label = 1 if label==3 else 0\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tench': 0,\n",
       " 'Tinca tinca': 0,\n",
       " 'English springer': 1,\n",
       " 'English springer spaniel': 1,\n",
       " 'cassette player': 2,\n",
       " 'chain saw': 3,\n",
       " 'chainsaw': 3,\n",
       " 'church': 4,\n",
       " 'church building': 4,\n",
       " 'French horn': 5,\n",
       " 'horn': 5,\n",
       " 'garbage truck': 6,\n",
       " 'dustcart': 6,\n",
       " 'gas pump': 7,\n",
       " 'gasoline pump': 7,\n",
       " 'petrol pump': 7,\n",
       " 'island dispenser': 7,\n",
       " 'golf ball': 8,\n",
       " 'parachute': 9,\n",
       " 'chute': 9}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebalance classes\n",
    "# train_indices = [idx for idx, (image, label) in enumerate(data_train) if label==1 or label==4 or label==3]\n",
    "# val_indices = [idx for idx, (image, label) in enumerate(data_val) if label==1 or label==4 or label==3]\n",
    "\n",
    "# Ignore this for now because we're not actually training\n",
    "train_indices = np.arange(len(data_train))\n",
    "val_indices = np.arange(len(data_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_chainsaw = ImagenetteDataset(data_train, train_indices)\n",
    "data_val_chainsaw = ImagenetteDataset(data_val, val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(data_train_chainsaw, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(data_val_chainsaw, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model import Resnet18, CNNClassifier\n",
    "\n",
    "# model = CNNClassifier(num_classes=2)\n",
    "model = Resnet18()\n",
    "lr = 0.008\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    running_loss = 0.\n",
    "    running_steps = 0\n",
    "    n_correct = 0\n",
    "    n_total = 0\n",
    "\n",
    "    with tqdm(data_loader, desc =\"   train\") as train_tqdm:\n",
    "        for inputs, targets in train_tqdm:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_steps += 1\n",
    "\n",
    "            _, pred = torch.max(outputs, 1)\n",
    "            n_correct += (pred==targets).sum().item()\n",
    "            n_total += len(targets)\n",
    "\n",
    "            average_train_loss = running_loss / running_steps\n",
    "            accuracy = n_correct / n_total * 100.\n",
    "\n",
    "            train_tqdm.set_postfix(loss=average_train_loss, accuracy=accuracy)\n",
    "\n",
    "    return average_train_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, data_loader, criterion, optimizer):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    running_loss = 0.\n",
    "    running_steps = 0\n",
    "    n_correct = 0\n",
    "    n_total = 0\n",
    "\n",
    "    with tqdm(data_loader, desc =\"   test\") as test_tqdm:\n",
    "        for inputs, targets in test_tqdm:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item()\n",
    "            running_steps += 1\n",
    "\n",
    "            _, pred = torch.max(outputs, 1)\n",
    "            n_correct += (pred==targets).sum().item()\n",
    "            n_total += len(targets)\n",
    "    \n",
    "        average_test_loss = running_loss / running_steps\n",
    "        accuracy = n_correct / n_total * 100.\n",
    "\n",
    "        test_tqdm.set_postfix(loss=average_test_loss, accuracy=accuracy)\n",
    "\n",
    "    return average_test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0):\n",
    "    train_avg_loss, train_accuracy = train(model, train_loader, criterion, optimizer)\n",
    "    if (i+1)%10 == 0:\n",
    "        val_avg_loss, val_accuracy = test(model, val_loader, criterion, optimizer)\n",
    "        print('Epoch: {:3d}, Train Average Loss: {:.2f}, Train Accuracy: {:.1f}%, Validation Average Loss: {:.2f}, Validation Accuracy: {:.1f}%'\\\n",
    "              .format(i+1, train_avg_loss, train_accuracy, val_avg_loss, val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_avg_loss, val_accuracy = test(model, val_loader, criterion, optimizer)\n",
    "print('Validation Average Loss: {:.2f}, Validation Accuracy: {:.1f}%'\\\n",
    "             .format(val_avg_loss, val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './models/default_imagenet.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
